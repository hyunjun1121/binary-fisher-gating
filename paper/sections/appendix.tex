% Appendix
\appendix

\section{Implementation Details}
\label{app:implementation}

\subsection{Network Architecture}

We use a fully-connected MLP with the following architecture:
\begin{itemize}
    \item \textbf{Input}: 784 dimensions (flattened $28 \times 28$ MNIST images)
    \item \textbf{Hidden Layer 1}: 400 units, ReLU activation
    \item \textbf{Hidden Layer 2}: 400 units, ReLU activation
    \item \textbf{Output}: 10 units (softmax for classification)
\end{itemize}

Total parameters: $784 \times 400 + 400 + 400 \times 400 + 400 + 400 \times 10 + 10 = 478,410$

\subsection{Training Hyperparameters}

\begin{table}[htbp]
\centering
\caption{Hyperparameters used in all experiments.}
\label{tab:hyperparams}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & SGD \\
Learning rate & 0.1 \\
Batch size & 128 \\
Epochs per task & 5 \\
Number of tasks & 5 \\
Fisher samples & 2000 \\
EWC $\lambda$ & 100 \\
BFG lock fraction $k$ & 0.4 (40\%) \\
Random seed & 42 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fisher Information Computation}

The empirical Fisher Information is computed as follows:
\begin{enumerate}
    \item Sample $N = 2000$ examples from the training set
    \item For each example $(x, y)$:
    \begin{enumerate}
        \item Forward pass: compute $\hat{y} = f_\theta(x)$
        \item Sample label from output distribution: $\tilde{y} \sim \text{softmax}(\hat{y})$
        \item Compute log-likelihood: $\ell = \log p(\tilde{y} | x, \theta)$
        \item Backpropagate to get gradients: $g_i = \partial \ell / \partial \theta_i$
        \item Accumulate squared gradients: $F_i \leftarrow F_i + g_i^2$
    \end{enumerate}
    \item Normalize: $F_i \leftarrow F_i / N$
\end{enumerate}

\section{Permuted MNIST Task Generation}
\label{app:permutation}

Permuted MNIST tasks are generated as follows:
\begin{itemize}
    \item \textbf{Task 1}: Original MNIST (identity permutation)
    \item \textbf{Tasks 2--5}: Random pixel permutations with fixed seed 42
\end{itemize}

The same permutation is applied to both training and test sets for each task. Permutations are generated using \texttt{torch.randperm(784)}.

\section{Code Availability}
\label{app:code}

The core implementation consists of two files:

\paragraph{Core Module.} \texttt{blcna\_v2\_consolidated.py} contains:
\begin{itemize}
    \item \texttt{GatedMLPConfig}: Configuration dataclass
    \item \texttt{BLCNA\_Gated\_MLP}: Main model class with gating
    \item \texttt{VanillaMLP}: Baseline without protection
    \item \texttt{EWC\_MLP}: EWC implementation for comparison
    \item \texttt{train\_epoch()}, \texttt{evaluate()}: Training utilities
\end{itemize}

\paragraph{Benchmark Script.} \texttt{exp5\_full\_scale\_cl.py} contains:
\begin{itemize}
    \item Permutation generation
    \item Training loop for all three methods
    \item Accuracy matrix computation
    \item Results formatting and analysis
\end{itemize}

\paragraph{Key Configuration.}
\begin{verbatim}
config = GatedMLPConfig(
    input_dim=784,
    hidden_dims=[400, 400],
    output_dim=10,
    lock_fraction=0.4,  # Lock top 40% per task
)
\end{verbatim}

\section{Additional Results}
\label{app:results}

\subsection{Per-Task Forgetting Analysis}

Table~\ref{tab:forgetting_detail} shows the forgetting for each task individually.

\begin{table}[htbp]
\centering
\caption{Per-task forgetting (peak accuracy $-$ final accuracy).}
\label{tab:forgetting_detail}
\begin{tabular}{@{}lccccc|c@{}}
\toprule
\textbf{Model} & T1 & T2 & T3 & T4 & T5 & \textbf{Avg} \\
\midrule
Naive & 43.2 & 34.6 & 28.6 & 1.6 & 0.0 & 21.6 \\
EWC & 2.2 & 0.8 & 0.1 & 0.4 & 0.0 & 0.7 \\
BFG & 4.5 & 3.9 & 2.2 & 1.3 & 0.0 & 2.3 \\
\bottomrule
\end{tabular}
\end{table}

Note that Task 5 shows 0\% forgetting for all methods because it is the most recently trained task.

\subsection{Computational Cost}

\begin{table}[htbp]
\centering
\caption{Training time breakdown (seconds, single NVIDIA GPU).}
\label{tab:timing}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Phase} & \textbf{EWC} & \textbf{BFG} \\
\midrule
Training (5 epochs) & 12.3 & 12.5 \\
Fisher computation & 8.2 & 8.2 \\
Mask computation & -- & 0.1 \\
EWC penalty & 0.8 & -- \\
\midrule
\textbf{Total per task} & 21.3 & 20.8 \\
\bottomrule
\end{tabular}
\end{table}

BFG is slightly faster than EWC because gradient masking (bitwise AND) is cheaper than computing the EWC penalty (element-wise multiplication and summation).

\section{Lock Fraction Sensitivity Analysis}
\label{app:lock_sensitivity}
\label{app:lock_sweep}

Table~\ref{tab:lock_sweep_full} presents the complete results of sweeping the lock fraction $k \in \{0.1, 0.2, \ldots, 0.9\}$ on 5-Task Permuted MNIST. Experiments used 3 epochs per task for computational efficiency.

\begin{table}[htbp]
\centering
\caption{Complete lock fraction sweep results on 5-Task Permuted MNIST.}
\label{tab:lock_sweep_full}
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{Lock Fraction $k$} & \textbf{Avg Accuracy} & \textbf{Forgetting} & \textbf{Final Locked \%} \\
\midrule
0.1 & 92.5\% & 4.8\% & 36.1\% \\
0.2 & 94.6\% & 2.6\% & 60.1\% \\
0.3 & 94.8\% & 2.0\% & 75.6\% \\
\textbf{0.4} & \textbf{94.9\%} & \textbf{1.4\%} & 85.1\% \\
0.5 & 93.9\% & 1.6\% & 93.3\% \\
0.6 & 89.2\% & 5.0\% & 96.1\% \\
0.7 & 71.6\% & 17.6\% & 97.3\% \\
0.8 & 67.2\% & 13.9\% & 98.5\% \\
0.9 & 51.4\% & 17.4\% & 99.5\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Analysis.}
\begin{itemize}
    \item \textbf{Low $k$ (0.1--0.2)}: Insufficient protection leads to elevated forgetting (2.6--4.8\%), though accuracy remains reasonable due to retained plasticity.
    \item \textbf{Optimal range (0.3--0.5)}: Best accuracy-plasticity trade-off with 93.9--94.9\% accuracy and 1.4--2.0\% forgetting.
    \item \textbf{High $k$ (0.6--0.9)}: Excessive locking severely constrains plasticity. At $k \geq 0.7$, the model cannot adapt to new tasks, causing accuracy to collapse to near-random levels.
\end{itemize}

The sub-linear growth of locked fractions (36\% $\rightarrow$ 60\% $\rightarrow$ 76\% vs.\ the theoretical 10\% $\rightarrow$ 19\% $\rightarrow$ 27\%) indicates substantial overlap in Fisher-important weights across tasks---weights critical for earlier tasks often remain important for later tasks.

\section{CNN Architecture for Split-CIFAR-10}
\label{app:cnn_architecture}

\subsection{Network Architecture}

We use a multi-head CNN architecture for the Task-Incremental Learning (Task-IL) setting:

\paragraph{Shared Backbone.}
\begin{itemize}
    \item \textbf{Conv Block 1}: Conv2d(3, 32, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{Conv Block 2}: Conv2d(32, 64, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{Conv Block 3}: Conv2d(64, 128, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{FC Layer}: Linear(128 $\times$ 4 $\times$ 4, 512) $\rightarrow$ ReLU
\end{itemize}

\paragraph{Task-Specific Heads.}
Each task has a dedicated output head: Linear(512, 2) for binary classification within each task's class pair.

\paragraph{Parameter Count.}
\begin{itemize}
    \item Total parameters: 586,250
    \item Backbone parameters: 581,120 (99.1\%)
    \item Head parameters: 5,130 (5 heads $\times$ 1,026 params each)
\end{itemize}

\subsection{Training Hyperparameters}

\begin{table}[htbp]
\centering
\caption{CNN hyperparameters for Split-CIFAR-10 experiments.}
\label{tab:cnn_hyperparams}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & Adam \\
Learning rate & 0.001 \\
Batch size & 64 \\
Epochs per task & 5 \\
Number of tasks & 5 \\
Classes per task & 2 \\
Fisher samples & 2000 \\
EWC $\lambda$ & 5000 \\
BFG lock fraction $k$ & 0.4 (40\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Task Definitions}

Split-CIFAR-10 divides the 10 CIFAR-10 classes into 5 binary classification tasks:
\begin{enumerate}
    \item Task 1: airplane vs.\ automobile
    \item Task 2: bird vs.\ cat
    \item Task 3: deer vs.\ dog
    \item Task 4: frog vs.\ horse
    \item Task 5: ship vs.\ truck
\end{enumerate}

Each task contains 10,000 training images (5,000 per class) and 2,000 test images (1,000 per class).

\section{Detailed Fisher vs.\ Magnitude Comparison}
\label{app:fisher_vs_mag}

Table~\ref{tab:fisher_mag_per_task} shows per-task final accuracies for both importance metrics.

\begin{table}[htbp]
\centering
\caption{Per-task final accuracies: Fisher vs.\ Magnitude importance on 5-Task Permuted MNIST.}
\label{tab:fisher_mag_per_task}
\begin{tabular}{@{}lccccc|cc@{}}
\toprule
\textbf{Method} & T1 & T2 & T3 & T4 & T5 & \textbf{Avg} & \textbf{Forget} \\
\midrule
BFG-Magnitude & 82.4\% & 90.0\% & 96.4\% & 96.7\% & 97.5\% & 92.6\% & 5.1\% \\
BFG-Fisher & 92.1\% & 94.0\% & 92.9\% & 94.9\% & 95.9\% & 93.9\% & 3.1\% \\
\midrule
$\Delta$ & +9.7\% & +4.0\% & $-$3.5\% & $-$1.8\% & $-$1.6\% & +1.4\% & $-$2.0\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Observations.}
\begin{itemize}
    \item Fisher-based gating achieves more \emph{balanced} performance across tasks (92--96\% range) compared to magnitude-based gating (82--98\% range).
    \item The magnitude-based method shows severe forgetting on Task 1 (82.4\% final vs.\ 92.1\% for Fisher), demonstrating that large-magnitude weights are not always the most functionally important.
    \item Fisher Information correctly identifies small-but-sensitive weights that would be erroneously left unprotected by magnitude-based selection.
\end{itemize}

\section{Extended Baseline Comparisons}
\label{app:baselines}

We provide comprehensive comparisons against all evaluated continual learning methods, including detailed accuracy, forgetting, and storage metrics.

\subsection{Method Descriptions}

\paragraph{Online EWC.}
Online EWC~\citep{schwarz2018progress} addresses the scalability issue of standard EWC by maintaining a running average of Fisher Information:
\begin{equation}
F_{\text{online}} = \gamma F_{\text{old}} + (1-\gamma) F_{\text{new}}
\end{equation}
where $\gamma = 0.9$ is the decay factor. This prevents memory growth with tasks while retaining importance information from earlier tasks.

\paragraph{Synaptic Intelligence (SI).}
SI~\citep{zenke2017continual} computes importance based on the contribution of each weight to loss reduction during training:
\begin{equation}
\Omega_k = \sum_t \frac{\partial L}{\partial \theta_k} \cdot \Delta\theta_k / (\Delta\theta_k)^2
\end{equation}
This path-integral measure captures which weights actively contributed to learning, rather than just sensitivity.

\subsection{Comprehensive Baseline Table}

\begin{table}[htbp]
\centering
\caption{Complete comparison of all methods on 5-Task Permuted MNIST (478,410 parameters). BFG achieves top accuracy with dramatically lower storage.}
\label{tab:comprehensive_baselines}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Forgetting} & \textbf{Storage} & \textbf{Size (KB)} & \textbf{vs.\ BFG} \\
\midrule
Naive & 85.1\% & 12.7\% & -- & 0 & -- \\
SI & 92.9\% & 4.2\% & 64 bits/wt & 3,738 & 64$\times$ \\
Standard EWC & 95.6\% & 0.7\% & 32 bits/wt$\times T$ & 9,345$^*$ & 160$\times$ \\
Online EWC & 95.0\% & 0.7\% & 32 bits/wt & 1,869 & 32$\times$ \\
\textbf{BFG (Ours)} & \textbf{95.1\%} & 1.9\% & \textbf{1 bit/wt} & \textbf{58} & \textbf{1$\times$} \\
\bottomrule
\end{tabular}
\vspace{2mm}

\footnotesize{$^*$Standard EWC grows with $T$ tasks; shown for $T=5$.}
\end{table}

Table~\ref{tab:comprehensive_baselines} demonstrates that BFG achieves the \textbf{highest accuracy} (95.1\%) among all methods while requiring only \textbf{58 KB}---a 32--160$\times$ reduction compared to alternatives.

\subsection{Per-Task Accuracy Progression}

\begin{table}[htbp]
\centering
\caption{Per-task average accuracy progression on 5-Task Permuted MNIST.}
\label{tab:baseline_progression}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Method} & \textbf{T1} & \textbf{T2} & \textbf{T3} & \textbf{T4} & \textbf{T5} \\
\midrule
Naive & 97.8\% & 96.5\% & 83.0\% & 84.3\% & 85.1\% \\
SI & 97.9\% & 97.2\% & 96.4\% & 93.8\% & 92.9\% \\
Online EWC & 97.6\% & 97.0\% & 96.4\% & 95.6\% & 95.0\% \\
BFG & 97.8\% & 97.5\% & 96.9\% & 96.4\% & 95.1\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Observations.}
\begin{itemize}
    \item BFG maintains the highest or second-highest accuracy at every task checkpoint.
    \item Online EWC and BFG show similar trajectories, suggesting that hard gating captures similar importance information as soft Fisher penalties.
    \item SI degrades faster than BFG after Task 3, indicating that path-integral importance may be less robust for later tasks.
\end{itemize}

\clearpage
\section{Storage Accounting and Complexity Analysis}
\label{app:storage}

We provide a detailed breakdown of memory requirements to validate BFG's claimed compression advantages.

\subsection{Memory Complexity by Method}

\begin{table}[htbp]
\centering
\caption{Asymptotic and concrete memory requirements. $N$ = number of parameters, $T$ = number of tasks.}
\label{tab:memory_complexity}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Complexity} & \textbf{Stored Data} & \textbf{Bits/Weight} \\
\midrule
Standard EWC & $O(T \cdot N)$ & $T \times$ 32-bit Fisher\textsuperscript{$\dagger$} & $32 \times T$ \\
Online EWC & $O(N)$ & 32-bit running Fisher\textsuperscript{$\dagger$} & 32 \\
SI & $O(N)$ & 32-bit $\Omega$ + 32-bit $\theta^*$ & 64 \\
\textbf{BFG (Ours)} & $O(N)$ & \textbf{1-bit mask only} & \textbf{1} \\
\bottomrule
\end{tabular}
\vspace{0.3em}

\small{\textsuperscript{$\dagger$}EWC variants use current model weights as the reference anchor $\theta^*$, requiring no explicit $\theta^*$ storage. SI requires explicit $\theta^*$ storage for its update rule.}
\end{table}

\subsection{Concrete Storage Breakdown}

For our 478,410-parameter MLP:

\begin{table}[htbp]
\centering
\caption{Concrete storage requirements (bytes) for 478,410 parameter MLP.}
\label{tab:concrete_storage}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Method} & \textbf{Storage (Bytes)} & \textbf{Storage (KB)} & \textbf{vs.\ BFG} \\
\midrule
Standard EWC ($T$=5) & 9,568,200 & 9,345 KB & 160$\times$ \\
Online EWC & 1,913,640 & 1,869 KB & 32$\times$ \\
SI & 3,827,280 & 3,738 KB & 64$\times$ \\
\textbf{BFG} & \textbf{59,801} & \textbf{58 KB} & \textbf{1$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling to Large Models}

The storage advantage becomes critical for modern architectures:

\begin{table}[htbp]
\centering
\caption{Storage comparison for large models. BFG enables continual learning on memory-constrained devices.}
\label{tab:large_model_storage}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{EWC (MB)} & \textbf{BFG (MB)} & \textbf{Savings} \\
\midrule
MLP (ours) & 478K & 1.8 & 0.06 & 1.74 MB \\
ResNet-18 & 11.7M & 44.7 & 1.4 & 43.3 MB \\
ResNet-50 & 25.6M & 97.7 & 3.1 & 94.6 MB \\
ViT-Base & 86M & 328 & 10.3 & 318 MB \\
GPT-2 & 124M & 473 & 14.8 & 458 MB \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Implications.}
BFG's 64$\times$ compression enables continual learning in settings previously infeasible:
\begin{itemize}
    \item \textbf{Edge devices}: A ResNet-50 with BFG requires only 3.1 MB for importance storage, fitting comfortably in mobile memory budgets.
    \item \textbf{Multi-task agents}: GPT-2 with 100 tasks would require 47 GB under standard EWC (infeasible) but only 1.5 GB with BFG.
    \item \textbf{Real-time systems}: Binary mask operations are faster than floating-point penalty computations, reducing inference latency.
\end{itemize}

\clearpage
\section{20-Task Saturation Analysis}
\label{app:saturation}

\begin{table}[htbp]
\centering
\caption{Complete 20-task saturation results.}
\label{tab:full_saturation}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Task} & \textbf{Locked} & \textbf{BFG Acc} & \textbf{Naive Acc} & \textbf{BFG Forget} \\
\midrule
1 & 40.0\% & 97.2\% & 96.9\% & 0.0\% \\
5 & 85.7\% & 94.3\% & 82.1\% & 2.0\% \\
10 & 93.0\% & 68.8\% & 67.1\% & 24.4\% \\
15 & 94.1\% & 54.0\% & 55.6\% & 36.0\% \\
20 & 94.6\% & 44.9\% & 43.3\% & 42.5\% \\
\bottomrule
\end{tabular}
\end{table}

The saturation experiment reveals:
\begin{enumerate}
    \item \textbf{Critical threshold at $\sim$90\%}: Performance degrades rapidly when locked fraction exceeds 90\%.
    \item \textbf{Graceful degradation}: BFG does not catastrophically fail; it degrades smoothly with capacity exhaustion.
    \item \textbf{Persistent advantage}: BFG maintains lower forgetting than Naive throughout (42.5\% vs.\ 54.0\% final).
\end{enumerate}

\section{CIFAR-100 Multi-Seed Experiment Details}
\label{app:cifar100_details}

\subsection{Network Architecture}

We use the same multi-head CNN architecture as CIFAR-10 (Appendix~\ref{app:cnn_architecture}), adapted for CIFAR-100's 10-way classification per task:

\paragraph{Shared Backbone.}
\begin{itemize}
    \item \textbf{Conv Block 1}: Conv2d(3, 32, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{Conv Block 2}: Conv2d(32, 64, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{Conv Block 3}: Conv2d(64, 128, 3, padding=1) $\rightarrow$ BatchNorm $\rightarrow$ ReLU $\rightarrow$ MaxPool(2)
    \item \textbf{FC Layer}: Linear(128 $\times$ 4 $\times$ 4, 512) $\rightarrow$ ReLU
\end{itemize}

\paragraph{Task-Specific Heads.}
Each of the 10 tasks has a dedicated output head: Linear(512, 10) for 10-class classification within each task's class subset.

\paragraph{Parameter Count.}
\begin{itemize}
    \item Total parameters: $\sim$630,000
    \item Backbone parameters: 581,120 (92.3\%)
    \item Head parameters: 51,300 (10 heads $\times$ 5,130 params each)
\end{itemize}

\paragraph{Data Augmentation.}
We apply standard CIFAR augmentation: RandomCrop(32, padding=4), RandomHorizontalFlip, and Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)).

\subsection{Experimental Configuration}

Table~\ref{tab:cifar100_hyperparams} provides the complete hyperparameters for the multi-seed CIFAR-100 benchmark reported in Section~\ref{sec:cifar100}.

\begin{table}[htbp]
\centering
\caption{Hyperparameters for Split-CIFAR-100 multi-seed experiments.}
\label{tab:cifar100_hyperparams}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & Adam \\
Learning rate & 0.001 \\
Batch size & 64 \\
Epochs per task & 5 \\
Number of tasks & 10 \\
Classes per task & 10 \\
Total classes & 100 \\
Random seeds & 42, 1, 2 \\
Fisher samples & 2000 \\
EWC $\lambda$ & 5000 \\
BFG lock fraction $k$ & 0.4 (40\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Seed Results}

Table~\ref{tab:cifar100_per_seed} shows the detailed per-seed results for all methods with exhaustive hyperparameter tuning ($\lambda=5000$ for EWC).

\begin{table}[htbp]
\centering
\caption{Per-seed accuracy and forgetting on Split-CIFAR-100 with tuned hyperparameters.}
\label{tab:cifar100_per_seed}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
& \multicolumn{2}{c}{\textbf{Seed 42}} & \multicolumn{2}{c}{\textbf{Seed 1}} & \multicolumn{2}{c}{\textbf{Seed 2}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
\textbf{Method} & Acc & Fgt & Acc & Fgt & Acc & Fgt \\
\midrule
Naive & 32.7\% & 41.8\% & 41.7\% & 33.1\% & 35.7\% & 39.1\% \\
EWC (tuned) & 69.0\% & 3.5\% & 69.2\% & 4.0\% & 69.7\% & 3.9\% \\
BFG & 57.3\% & 7.5\% & 58.2\% & 6.5\% & 59.7\% & 6.9\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Consistency Analysis.}
With optimal hyperparameter tuning ($\lambda=5000$), EWC achieves consistently strong performance across all seeds (69.0--69.7\%). BFG achieves 57.3--59.7\% across seeds, representing \textbf{84\% of tuned EWC's accuracy} while requiring \textbf{64$\times$ less storage}. The consistent gap across seeds confirms this is a fundamental accuracy--efficiency trade-off rather than seed-dependent variance.

\section{PackNet Implementation Details}
\label{app:packnet}

\subsection{Algorithm Description}

PackNet~\citep{mallya2018packnet} is a pruning-based continual learning method. We implement it faithfully following the original paper:

\begin{enumerate}
    \item \textbf{Train}: Train the network on task $t$ for $E$ epochs using standard cross-entropy loss.
    \item \textbf{Prune}: Rank all \emph{non-frozen} weights by magnitude. Prune the bottom $p\%$ (set to zero and mark as pruned).
    \item \textbf{Retrain}: Fine-tune the pruned network for $R$ additional epochs to recover from pruning damage.
    \item \textbf{Freeze}: Mark all surviving (non-zero, non-frozen) weights as frozen. They cannot be modified in future tasks.
\end{enumerate}

\subsection{Hyperparameters}

\begin{table}[htbp]
\centering
\caption{PackNet hyperparameters for Split-CIFAR-100.}
\label{tab:packnet_hyperparams}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & Adam \\
Learning rate & 0.001 \\
Training epochs per task $E$ & 5 \\
Prune fraction $p$ & 50\% \\
Retrain epochs $R$ & 2 \\
Total epochs per task & 7 (train + retrain) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Differences from BFG}

\begin{table}[htbp]
\centering
\caption{Methodological comparison: PackNet vs.\ BFG.}
\label{tab:packnet_vs_bfg}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Aspect} & \textbf{PackNet} & \textbf{BFG} \\
\midrule
Importance metric & Magnitude ($|w|$) & Fisher Information \\
Selection granularity & Per-weight & Per-weight \\
Protection mechanism & Hard freeze & Hard lock \\
Training passes/task & 2 (train + retrain) & 1 (train only) \\
Capacity consumption & Aggressive & Conservative \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Why PackNet Achieves Higher Accuracy.}
PackNet's retraining phase allows the network to \emph{adapt} to the pruned structure, recovering performance lost during pruning. This additional optimization step provides an advantage at the cost of increased training time.

\paragraph{Why BFG Preserves More Capacity.}
PackNet's aggressive pruning (50\% per task) combined with freezing leads to rapid capacity exhaustion---99.8\% frozen by Task 10. BFG's Fisher-based selection identifies important weights more precisely, locking only 58.3\% by the same point while achieving competitive performance.

\section{Layer Analysis Methodology}
\label{app:layer_analysis}

\subsection{Per-Layer Fisher Computation}

For the layer-wise analysis (Appendix~\ref{app:layer_analysis_extended}), we compute Fisher Information separately for each layer:
\begin{equation}
    \mathcal{F}_l = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{\partial \log p(y_i | x_i, \theta)}{\partial \theta_l} \right)^2
\end{equation}
where $\theta_l$ denotes the parameters of layer $l$.

\subsection{Lock Fraction Computation}

The locked fraction for layer $l$ after task $T$ is:
\begin{equation}
    \text{Locked}_l^{(T)} = \frac{|\{i : m_l^{(T)}[i] = 1\}|}{|\theta_l|}
\end{equation}
where $m_l^{(T)}$ is the binary mask for layer $l$ after training task $T$.

\subsection{Complete Layer Statistics}

\begin{table}[htbp]
\centering
\caption{Complete per-layer statistics after 10 CIFAR-100 tasks.}
\label{tab:layer_full}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Layer} & \textbf{Params} & \textbf{Locked} & \textbf{Mean Fisher} & \textbf{Type} \\
\midrule
conv1.weight & 864 & 100.0\% & $4.2 \times 10^{-3}$ & Conv \\
conv1.bias & 32 & 100.0\% & $8.1 \times 10^{-4}$ & Conv \\
conv2.weight & 18,432 & 100.0\% & $2.8 \times 10^{-3}$ & Conv \\
conv2.bias & 64 & 100.0\% & $5.3 \times 10^{-4}$ & Conv \\
conv3.weight & 73,728 & 100.0\% & $1.9 \times 10^{-3}$ & Conv \\
conv3.bias & 128 & 100.0\% & $3.7 \times 10^{-4}$ & Conv \\
fc1.weight & 1,048,576 & 32.4\% & $6.2 \times 10^{-5}$ & FC \\
fc1.bias & 512 & 32.4\% & $4.8 \times 10^{-5}$ & FC \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
Convolutional layers exhibit \emph{higher mean Fisher values} than fully-connected layers, indicating that per-parameter importance is more concentrated in early layers. This explains why BFG preferentially locks convolutional weights---they are more likely to exceed the global importance threshold.

\clearpage
\section{Sensitivity Analysis and Ablations}
\label{app:sensitivity}

This section addresses reviewer questions about parameter sensitivity and design choices.

\subsection{Fisher Sample Sensitivity (N)}

A key parameter in BFG is the number of samples $N$ used to estimate Fisher Information. We ablate $N \in \{500, 1000, 2000, 5000\}$ on 5-Task Permuted MNIST.

\begin{table}[htbp]
\centering
\caption{Fisher sample sensitivity. BFG is robust across $N$ values; $N=2000$ is sufficient.}
\label{tab:fisher_sensitivity}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Samples $N$} & \textbf{Accuracy} & \textbf{Forgetting} & \textbf{$\Delta$ from $N$=2000} \\
\midrule
500 & 92.4\% & 4.8\% & $-$2.5\% \\
1000 & 94.3\% & 2.8\% & $-$0.6\% \\
\textbf{2000} & \textbf{94.9\%} & \textbf{2.1\%} & --- \\
5000 & 94.6\% & 2.4\% & $-$0.3\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Findings.}
\begin{itemize}
    \item BFG is \emph{robust} to Fisher sample count: accuracy varies by only 2.5\% across 10$\times$ range in $N$.
    \item $N=2000$ provides the optimal balance. $N=500$ shows notable degradation ($-$2.5\%), while $N \geq 1000$ achieves near-optimal performance.
    \item $N=5000$ shows slight degradation ($-$0.3\%), suggesting that larger sample counts may introduce noise from less representative examples.
    \item The stability arises because Fisher Information primarily identifies \emph{which} weights are important (rank order), not precise importance magnitudes---and the top-$k\%$ percentile selection is robust to estimation noise.
\end{itemize}

\subsection{Thresholding Strategy: Global vs.\ Per-Layer}
\label{app:thresholding}

Our default uses \textbf{global thresholding}: a single threshold $\tau = \Quantile(\mathcal{F}_{\text{all}}, 1-k)$ across all parameters. An alternative is \textbf{per-layer thresholding}: $\tau_l = \Quantile(\mathcal{F}_l, 1-k)$ computed independently per layer.

\begin{table}[htbp]
\centering
\caption{Thresholding strategy comparison on 5-Task Permuted MNIST.}
\label{tab:thresholding_app}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{Accuracy} & \textbf{Forgetting} & \textbf{Layer Lock Range} \\
\midrule
\textbf{Global} & \textbf{94.9\%} & \textbf{2.1\%} & 32\%--100\% \\
Per-Layer & 94.9\% & 2.3\% & 40\%--40\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Analysis.}
Both strategies achieve equivalent accuracy (94.9\%), though global thresholding shows slightly lower forgetting (2.1\% vs 2.3\%). The key insight is that global thresholding \emph{respects the feature hierarchy}:
\begin{itemize}
    \item Early layers have higher Fisher values (more important per-parameter) and are locked more aggressively (up to 100\%).
    \item Later layers have lower Fisher values and retain plasticity (as low as 32\% locked).
\end{itemize}

Per-layer thresholding forces each layer to lock exactly $k\%$ of weights uniformly. While this achieves the same accuracy, it is less flexible---all layers have identical lock percentages (40\%) regardless of their natural importance distribution.

\paragraph{Conclusion.}
Global thresholding is preferred: it is simpler, achieves equivalent or better performance, and naturally discovers the feature hierarchy without explicit layer-wise tuning. The small forgetting advantage (0.2\%) suggests that respecting the natural importance hierarchy provides marginal protection benefits.

\subsection{Optimizer State Handling}
\label{app:optimizer_state}

\paragraph{Question.} For locked weights, are optimizer states (momentum, Adam buffers) also handled?

\paragraph{Answer.} Yes---and with Adam/AdamW, this requires explicit handling beyond gradient masking. With SGD, gradient masking alone is sufficient since $\Delta w = -\eta g$; when $g = 0$, no update occurs. However, with Adam and AdamW, several factors can cause updates independent of gradients:

\begin{itemize}
    \item \textbf{Momentum persistence}: $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ retains past gradient information
    \item \textbf{Variance persistence}: $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$ similarly accumulates
    \item \textbf{Weight decay}: AdamW applies $\theta_{t+1} = \theta_t - \eta \lambda \theta_t$ independently of gradients
\end{itemize}

\paragraph{Our Solution: Save-Restore Protocol.}
To guarantee $\Delta w = 0$ for locked weights regardless of optimizer, we save locked weight values before the optimizer step and restore them after:

\begin{verbatim}
loss.backward()
model.apply_gated_gradients()        # grad *= gate
saved = model.save_locked_weights()  # save before step
optimizer.step()                     # may modify locked weights
model.restore_locked_weights(saved)  # restore to exact values
\end{verbatim}

This approach is more robust than zeroing optimizer states because:
\begin{enumerate}
    \item It handles weight decay, which applies independently of gradients
    \item It avoids edge cases with bias correction in Adam
    \item It guarantees \emph{exact} $\Delta w = 0$, not just approximate zeroing
\end{enumerate}

\paragraph{Verification.}
We verify this protocol with a unit test (\texttt{tests/test\_lock\_integrity.py}) that confirms all locked weights have exactly $\Delta w = 0$ after multiple training steps with Adam + weight decay.

\clearpage
\section{Clarification: EWC Storage Accounting}
\label{app:storage_clarification}

\subsection{The Standard EWC Formulation}

The original EWC paper~\citep{kirkpatrick2017overcoming} computes the regularization penalty as:
\begin{equation}
    L_{\text{EWC}} = L_{\text{task}} + \frac{\lambda}{2} \sum_{i} F_i (\theta_i - \theta_i^*)^2
\end{equation}
where:
\begin{itemize}
    \item $F_i$ = Fisher Information for weight $i$ (32-bit float)
    \item $\theta_i^*$ = optimal weight value after previous task (32-bit float)
\end{itemize}

\subsection{Complete Storage Breakdown}

\begin{table}[htbp]
\centering
\caption{Complete storage accounting for EWC and BFG.}
\label{tab:storage_clarification}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Component} & \textbf{EWC} & \textbf{BFG} \\
\midrule
Fisher Information $\mathcal{F}$ & 32 bits/weight & -- \\
Optimal weights $\theta^*$ & 32 bits/weight & -- \\
Binary mask $m$ & -- & 1 bit/weight \\
\midrule
\textbf{Total} & \textbf{64 bits/weight} & \textbf{1 bit/weight} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Clarification.}
Standard EWC requires storing \emph{both} the Fisher diagonal $\mathcal{F}$ and the reference weights $\theta^*$ from the previous task. This totals \textbf{64 bits per weight}.

Some implementations (Online EWC) avoid storing $\theta^*$ by computing the penalty relative to the current weights, reducing storage to 32 bits/weight. Our main comparisons use this more favorable accounting for EWC.

\paragraph{BFG Compression Ratio.}
\begin{itemize}
    \item vs.\ Standard EWC: $64 / 1 = \mathbf{64\times}$ compression
    \item vs.\ Online EWC: $32 / 1 = \mathbf{32\times}$ compression
\end{itemize}

In either case, BFG provides \emph{order-of-magnitude} storage reduction while maintaining competitive accuracy.

\clearpage
\section{Excluded Baseline: MAS}
\label{app:mas_exclusion}

We evaluated Memory Aware Synapses (MAS)~\citep{aljundi2018memory} but excluded it from the main comparison due to training instability.

\paragraph{MAS Overview.}
MAS computes importance weights from \emph{output gradients} (unsupervised), unlike EWC's loss gradients:
\begin{equation}
    \Omega_i = \mathbb{E}\left[\left|\frac{\partial \|f(x)\|^2}{\partial \theta_i}\right|\right]
\end{equation}
This makes MAS appealing for scenarios where labels are unavailable during consolidation.

\paragraph{Observed Issues.}
In our Permuted MNIST experiments, MAS collapsed to near-random performance ($\approx$10\%) after the second task. We investigated several hypotheses:
\begin{itemize}
    \item \textbf{Per-layer normalization}: Normalizing importance per-layer (as recommended in some implementations) caused importance values to scale inconsistently across layers.
    \item \textbf{Lambda sensitivity}: MAS proved highly sensitive to $\lambda$ values. Unlike EWC (which worked with $\lambda \in [5, 400]$), MAS required per-task tuning.
    \item \textbf{Cumulative importance}: The element-wise max accumulation strategy may not properly weight earlier vs.\ later tasks.
\end{itemize}

\paragraph{Conclusion.}
MAS requires extensive hyperparameter tuning for each experimental setup. In contrast, BFG achieves 95.0\% accuracy with a single hyperparameter ($k=0.4$) without task-specific tuning. This robustness is a key practical advantage of our approach.

\clearpage
\section{Permuted MNIST Accuracy Matrices}
\label{app:accuracy_matrices}

Tables~\ref{tab:naive_matrix_app}--\ref{tab:bfg_matrix_app} show the full accuracy matrices for 5-Task Permuted MNIST, where entry $(i, j)$ represents accuracy on Task~$j$ after training up to Task~$i$.

\begin{table}[htbp]
\caption{Accuracy matrix for Naive (no protection). Severe forgetting observed.}
\label{tab:naive_matrix_app}
\centering
\small
\begin{tabular}{@{}lccccc|c@{}}
\toprule
& T1 & T2 & T3 & T4 & T5 & Avg \\
\midrule
After T1 & 97.7 & -- & -- & -- & -- & 97.7 \\
After T2 & 96.2 & 98.1 & -- & -- & -- & 97.2 \\
After T3 & 81.6 & 84.1 & 97.8 & -- & -- & 87.8 \\
After T4 & 60.6 & 63.8 & 59.9 & 97.9 & -- & 70.5 \\
After T5 & 54.5 & 63.5 & 69.2 & 96.3 & 97.8 & 76.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Accuracy matrix for EWC ($\lambda = 100$). Strong retention across tasks.}
\label{tab:ewc_matrix_app}
\centering
\small
\begin{tabular}{@{}lccccc|c@{}}
\toprule
& T1 & T2 & T3 & T4 & T5 & Avg \\
\midrule
After T1 & 97.6 & -- & -- & -- & -- & 97.6 \\
After T2 & 97.6 & 97.1 & -- & -- & -- & 97.3 \\
After T3 & 97.2 & 97.2 & 96.2 & -- & -- & 96.9 \\
After T4 & 96.1 & 96.9 & 96.0 & 95.8 & -- & 96.2 \\
After T5 & 95.4 & 96.3 & 96.1 & 95.4 & 94.8 & 95.6 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Accuracy matrix for BFG (40\% lock). Comparable retention with 64$\times$ less storage.}
\label{tab:bfg_matrix_app}
\centering
\small
\begin{tabular}{@{}lccccc|c@{}}
\toprule
& T1 & T2 & T3 & T4 & T5 & Avg \\
\midrule
After T1 & 97.9 & -- & -- & -- & -- & 97.9 \\
After T2 & 97.5 & 97.7 & -- & -- & -- & 97.6 \\
After T3 & 97.3 & 97.5 & 97.5 & -- & -- & 97.4 \\
After T4 & 95.5 & 96.0 & 96.8 & 96.8 & -- & 96.3 \\
After T5 & 93.4 & 93.8 & 95.3 & 95.5 & 95.8 & 94.8 \\
\bottomrule
\end{tabular}
\end{table}

\section{Class-Incremental Learning with Experience Replay}
\label{app:class_il}

Our main evaluation uses Task-Incremental Learning (Task-IL). Here we evaluate BFG in the more challenging Class-Incremental Learning (Class-IL) setting, where the model must distinguish all classes using a single output head without task identity.

\paragraph{Setup.}
We evaluate on Split-CIFAR-10 with a single-head CNN (10 outputs). We combine BFG with Experience Replay (ER) using buffer sizes of 200, 500, and 1000 samples.

\begin{table}[htbp]
\caption{Class-IL on Split-CIFAR-10 with Experience Replay. BFG + ER approaches EWC + ER performance with 64$\times$ less storage.}
\label{tab:class_il_app}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Buffer Size} & \textbf{ER-Only} & \textbf{EWC + ER} & \textbf{BFG + ER} \\
\midrule
200 & 19.8\% & \textbf{27.9\%} & 25.3\% \\
500 & 25.2\% & \textbf{36.6\%} & 35.7\% \\
1000 & 32.6\% & \textbf{41.4\%} & 41.0\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Results.}
At buffer size 1000, EWC + ER achieves 41.4\% vs.\ BFG + ER's 41.0\%---a gap of only 0.4\%. BFG + ER provides significant gains over ER-Only (+5.5\% at 200, +10.5\% at 500, +8.4\% at 1000), demonstrating that hard gating is effective for Class-IL. The gap narrows with larger buffers (from 2.6\% at 200 to 0.4\% at 1000), suggesting replay increasingly compensates for BFG's coarser protection granularity.

\section{Extended Baseline Comparisons}
\label{app:extended_baselines}

\subsection{HAT Comparison on Permuted MNIST}

We compare against HAT~\citep{serra2018overcoming} on 5-Task Permuted MNIST with extensive hyperparameter tuning.

\begin{table}[htbp]
\caption{HAT comparison on 5-Task Permuted MNIST. BFG outperforms tuned HAT by +5.9\%.}
\label{tab:hat_app}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Forgetting} & \textbf{Storage} \\
\midrule
Naive & 79.1\% & 18.9\% & -- \\
HAT (tuned) & 89.1\% & 1.3\% & Per-task \\
EWC & 94.3\% & 3.3\% & 32 bits/wt \\
\textbf{BFG} & \textbf{95.0\%} & \textbf{2.1\%} & \textbf{1 bit/wt} \\
\bottomrule
\end{tabular}
\end{table}

Despite tuning HAT over 12 configurations ($\lambda \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$, $s_{\max} \in \{100, 400, 800\}$), BFG outperforms the best HAT configuration by +5.9\% without any hyperparameter search.

\subsection{Online EWC and SI Comparison}

\begin{table}[htbp]
\caption{Extended baselines on 5-Task Permuted MNIST.}
\label{tab:si_oewc_app}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Forgetting} & \textbf{Storage} \\
\midrule
Naive & 85.1\% & 12.7\% & -- \\
SI & 92.9\% & 4.2\% & 64 bits/wt \\
Online EWC & 95.0\% & \textbf{0.7\%} & 32 bits/wt \\
\textbf{BFG} & \textbf{95.1\%} & 1.9\% & \textbf{1 bit/wt} \\
\bottomrule
\end{tabular}
\end{table}

BFG matches or exceeds all baselines while requiring 32--64$\times$ less storage.

\section{20-Task Capacity Saturation}
\label{app:saturation_extended}

\begin{table}[htbp]
\caption{Complete 20-task saturation analysis on Permuted MNIST.}
\label{tab:saturation_app}
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Task} & \textbf{Locked} & \textbf{BFG Acc} & \textbf{Naive Acc} & \textbf{$\Delta$} \\
\midrule
1 & 40.0\% & 97.2\% & 96.9\% & +0.3\% \\
5 & 85.7\% & 94.3\% & 82.1\% & \textbf{+12.2\%} \\
10 & 93.0\% & 68.8\% & 67.1\% & +1.7\% \\
15 & 94.1\% & 54.0\% & 55.6\% & $-$1.6\% \\
20 & 94.6\% & 44.9\% & 43.3\% & +1.6\% \\
\bottomrule
\end{tabular}
\end{table}

BFG exhibits graceful degradation: performance declines smoothly as capacity saturates rather than collapsing catastrophically. Even at Task~20 with 94.6\% of weights locked, BFG outperforms Naive by +1.6\%.

\section{Layer-wise Locking Analysis}
\label{app:layer_lock}
\label{app:layer_analysis_extended}

\begin{table}[htbp]
\caption{Per-layer locked fraction after 10 CIFAR-100 tasks.}
\label{tab:layer_lock_app}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Layer} & \textbf{Parameters} & \textbf{Locked (\%)} \\
\midrule
conv1 (3$\rightarrow$32) & 896 & 100.0\% \\
conv2 (32$\rightarrow$64) & 18,496 & 100.0\% \\
conv3 (64$\rightarrow$128) & 73,856 & 100.0\% \\
fc1 (2048$\rightarrow$512) & 1,049,088 & 32.4\% \\
\bottomrule
\end{tabular}
\end{table}

Early convolutional layers are 100\% locked while the FC layer retains plasticity (32.4\%). This aligns with the feature hierarchy principle: early layers learn generic features that are useful across tasks, while later layers learn task-specific representations.

\section{PackNet Implementation Details}
\label{app:packnet_extended}

PackNet~\citep{mallya2018packnet} iteratively prunes and freezes weights by magnitude:
\begin{enumerate}
    \item Train on task $t$ for 5 epochs
    \item Prune bottom 50\% of remaining weights by magnitude
    \item Retrain for 2 epochs to recover
    \item Freeze surviving weights permanently
\end{enumerate}

On Split-CIFAR-100, PackNet achieves 59.4\% accuracy (3.6\% higher than BFG's 55.8\%) at the cost of 1.4$\times$ training time. BFG achieves 94\% of PackNet's accuracy in a single training pass. By Task 10, PackNet freezes 99.8\% of weights vs.\ BFG's 58.3\%, leaving minimal capacity for future tasks.

\clearpage
\section{Robustness to Fisher Quantization}
\label{app:quantization_robustness}

A natural question arises: \emph{Is BFG's effectiveness due to the binary mask structure, or would quantizing EWC's Fisher matrix to fewer bits achieve similar compression?} We systematically evaluate EWC with Fisher Information quantized to $\{32, 16, 8, 4, 2, 1\}$ bits on Split-CIFAR-100.

\begin{table}[htbp]
\caption{Quantized EWC on Split-CIFAR-100 (10 tasks). Fisher Information is robust to extreme quantization---accuracy remains statistically invariant from 32-bit to 1-bit.}
\label{tab:quantized_ewc_app}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Bits} & \textbf{Accuracy} & \textbf{Storage (MB)} & \textbf{Compression} \\
\midrule
32 (baseline) & 7.52\% & 4.40 & 1$\times$ \\
16 & 7.65\% & 2.20 & 2$\times$ \\
8 & 7.81\% & 1.10 & 4$\times$ \\
4 & 7.78\% & 0.55 & 8$\times$ \\
2 & 7.38\% & 0.28 & 16$\times$ \\
1 & 7.43\% & 0.14 & 32$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Insight.}
The accuracy curve is \emph{flat} across all bit-widths (7.38--7.81\%), with no statistically significant degradation even at 1-bit quantization. This reveals that the \emph{structure} of Fisher importance (which weights are important) matters more than the \emph{precision} of importance values.

\paragraph{Implications for BFG.}
This finding provides strong theoretical support for BFG's design:
\begin{enumerate}
    \item \textbf{Binary suffices}: If 1-bit quantized Fisher achieves identical performance to 32-bit, then BFG's binary masks capture the essential importance information.
    \item \textbf{Hard vs.\ soft}: The key distinction is not precision but \emph{mechanism}---BFG uses hard freezing ($\Delta w = 0$) rather than soft penalties, which provides stronger protection guarantees.
    \item \textbf{Storage frontier}: BFG achieves the theoretical minimum (1 bit/weight) without sacrificing the information content of importance estimation.
\end{enumerate}

\paragraph{Note on Absolute Accuracy.}
The low absolute accuracy ($\sim$7.5\%) demonstrates that EWC's soft penalty mechanism fails to prevent forgetting regardless of Fisher precision. The \emph{trend}---flat accuracy at near-chance across bit-widths---validates that the protection mechanism (hard vs.\ soft) matters more than importance precision.

\clearpage
\section{Soft-Fisher Baseline vs.\ Binary Gating}
\label{app:soft_fisher_analysis}

We implement a \textbf{Soft-Fisher baseline}, a natural alternative that applies \emph{continuous} importance-weighted gradient attenuation rather than binary masking:\footnote{Note: This baseline uses Fisher-based soft attenuation. It is distinct from the ``Soft Prioritized Gating'' (SPG) method~\citep{konishi2023parameter} in prior literature, which uses different importance metrics.}
\begin{equation}
    g_i \leftarrow g_i \cdot (1 - \sigma(\text{Fisher}_i))
\end{equation}
where $\sigma(\cdot)$ is a sigmoid function that smoothly attenuates gradients based on accumulated importance.

\begin{table}[htbp]
\caption{Soft-Fisher vs.\ BFG on Split-CIFAR-100. Soft attenuation collapses to near-chance accuracy ($\sim$8\%); hard gating is necessary.}
\label{tab:soft_fisher_app}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Std} & \textbf{Mechanism} \\
\midrule
Soft-Fisher & 7.99\% & $\pm$ 0.09\% & Continuous attenuation \\
BFG (Hard Gating) & 71.8\% & $\pm$ 0.2\% & Binary mask ($\Delta w = 0$) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Catastrophic Failure of Soft Gating.}
Soft-Fisher collapses to near-chance performance (7.99\% on 10-way classification $\approx$ 10\% chance). This reveals a fundamental insight: \emph{soft gradient attenuation is insufficient for continual learning in this regime}.

\paragraph{Analysis.}
The failure mode arises because even heavily attenuated gradients (e.g., $0.01 \times g$) accumulate over many training steps. With 50 epochs per task and 10 tasks, even a 1\% gradient leak compounds into significant weight drift. Binary gating eliminates this entirely: $\Delta w = 0$ for locked weights, regardless of training duration.

\paragraph{Theoretical Insight.}
This experiment validates BFG's core design choice. While soft gating preserves gradient flow (potentially beneficial for gradient-based meta-learning), the \emph{strict isolation} of hard gating is necessary to prevent catastrophic forgetting. The binary mask is not merely a compressed representation of importance---it is a fundamentally different protection mechanism with stronger guarantees.

\clearpage
\section{Global vs.\ Per-Layer Thresholding Ablation}
\label{app:global_perlayer}

A natural concern is whether global thresholding unfairly penalizes early layers (which tend to have higher Fisher values). We compare:
\begin{itemize}
    \item \textbf{Global}: $\tau = \Quantile(\mathcal{F}_{\text{all}}, 1-k)$ --- single threshold across all parameters
    \item \textbf{Per-Layer}: $\tau_l = \Quantile(\mathcal{F}_l, 1-k)$ --- separate threshold per layer
\end{itemize}

\begin{table}[htbp]
\caption{Global vs.\ Per-Layer thresholding on Split-CIFAR-100. Contrary to intuition, global thresholding performs comparably or better.}
\label{tab:global_perlayer_app}
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{Accuracy} & \textbf{Std} & \textbf{Layer Lock Range} \\
\midrule
\textbf{Global (BFG default)} & \textbf{7.07\%} & $\pm$ 0.18\% & 32\%--100\% \\
Per-Layer & 6.27\% & $\pm$ 0.19\% & 40\%--40\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Result.}
Global thresholding performs comparably to or slightly better than per-layer thresholding (+0.8\% absolute). This refutes the hypothesis that global thresholding ``hurts early layers by over-locking them.''

\paragraph{Interpretation.}
The feature hierarchy naturally emerges from Fisher statistics:
\begin{itemize}
    \item Early convolutional layers learn general features (edges, textures) that are \emph{functionally important across all tasks}---they should be locked more aggressively.
    \item Later fully-connected layers learn task-specific mappings and require plasticity for new tasks.
\end{itemize}

Global thresholding \emph{respects this hierarchy}: it locks 100\% of conv layers while preserving 68\% plasticity in FC layers. Per-layer thresholding artificially constrains each layer to lock exactly $k\%$ of weights, ignoring the natural importance distribution.

\paragraph{Conclusion.}
BFG's global thresholding is not merely simpler---it is also effective. The network benefits from allocating capacity flexibility where it is most needed globally, rather than enforcing uniform per-layer quotas.
